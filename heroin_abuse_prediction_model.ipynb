{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At-Risk Heroin-Abuse Prediction Model Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains Python code to identify a human being as a potential abuser of heroin. Using the UCI dataset for drug consumption located, we are able to build a Wide-Deep Learning Model that combines the benefits of Linear Models with Sparse and Crossed Features and Deep Neural Networs.  \n",
    "\n",
    "The Drug Consumption data set with documentation can be located here: https://archive.ics.uci.edu/ml/datasets/Drug+consumption+(quantified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the required packages for Tensorflow and Data Engineering:  \n",
    "* [tensorflow](https://www.tensorflow.org)\n",
    "* [numpy](http://www.numpy.org)\n",
    "* [pandas](http://pandas.pydata.org)\n",
    "* [matplotlib](https://matplotlib.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from six.moves import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Clean Up the Data  \n",
    "First, we set some variables to manage where our data is located and where we want it to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00373'\n",
    "file_name = 'drug_consumption.data'\n",
    "file_url = '%s/%s' % (url, file_name)\n",
    "data_directory = '/tmp/drug_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the data, perform some cleaning on it, and save it to our data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If directory does not exist, make the directory.\n",
    "if not tf.gfile.Exists(data_directory):\n",
    "    tf.gfile.MkDir(data_directory)\n",
    "\n",
    "# Create a temporary file to hold the original.\n",
    "temp_file, _ = urllib.request.urlretrieve(file_url)\n",
    "clean_file = os.path.join(data_directory, file_name)\n",
    "\n",
    "# Read the temporary file and write the cleaned up data to the permanent file.\n",
    "with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n",
    "    with tf.gfile.Open(clean_file, 'w') as eval_file:\n",
    "      for line in temp_eval_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(', ', ',')\n",
    "        if not line or ',' not in line:\n",
    "          continue\n",
    "        if line[-1] == '.':\n",
    "          line = line[:-1]\n",
    "        line += '\\n'\n",
    "        eval_file.write(line)\n",
    "tf.gfile.Remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering and Exploration  \n",
    "Using a combination of pandas and matplotlib, we are able to parse our csv into a Dataframe object and visualize the data in the form of tables and plots.  \n",
    "\n",
    "First, we load the data into a Dataframe and display a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>n_score</th>\n",
       "      <th>e_score</th>\n",
       "      <th>o_score</th>\n",
       "      <th>a_score</th>\n",
       "      <th>c_score</th>\n",
       "      <th>...</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legal_h</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushroom</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.59171</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-1.22751</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>-0.30033</td>\n",
       "      <td>-1.55521</td>\n",
       "      <td>2.03972</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.09449</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>-1.09207</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>0.93949</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-1.73790</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-1.32828</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.62967</td>\n",
       "      <td>2.57309</td>\n",
       "      <td>-0.97631</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>1.13407</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.82213</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>-1.42424</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.12331</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    gender education   country ethnicity   n_score   e_score  \\\n",
       "id                                                                         \n",
       "1    0.49788   0.48246  -0.05921   0.96082   0.12600   0.31287  -0.57545   \n",
       "2   -0.07854  -0.48246   1.98437   0.96082  -0.31685  -0.67825   1.93886   \n",
       "3    0.49788  -0.48246  -0.05921   0.96082  -0.31685  -0.46725   0.80523   \n",
       "4   -0.95197   0.48246   1.16365   0.96082  -0.31685  -0.14882  -0.80615   \n",
       "5    0.49788   0.48246   1.98437   0.96082  -0.31685   0.73545  -1.63340   \n",
       "6    2.59171   0.48246  -1.22751   0.24923  -0.31685  -0.67825  -0.30033   \n",
       "7    1.09449  -0.48246   1.16365  -0.57009  -0.31685  -0.46725  -1.09207   \n",
       "8    0.49788  -0.48246  -1.73790   0.96082  -0.31685  -1.32828   1.93886   \n",
       "9    0.49788   0.48246  -0.05921   0.24923  -0.31685   0.62967   2.57309   \n",
       "10   1.82213  -0.48246   1.16365   0.96082  -0.31685  -0.24649   0.00332   \n",
       "\n",
       "     o_score   a_score   c_score ...  ecstasy heroin ketamine legal_h  lsd  \\\n",
       "id                               ...                                         \n",
       "1   -0.58331  -0.91699  -0.00665 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "2    1.43533   0.76096  -0.14277 ...      CL4    CL0      CL2     CL0  CL2   \n",
       "3   -0.84732  -1.62090  -1.01450 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "4   -0.01928   0.59042   0.58489 ...      CL0    CL0      CL2     CL0  CL0   \n",
       "5   -0.45174  -0.30172   1.30612 ...      CL1    CL0      CL0     CL1  CL0   \n",
       "6   -1.55521   2.03972   1.63088 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "7   -0.45174  -0.30172   0.93949 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "8   -0.84732  -0.30172   1.63088 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "9   -0.97631   0.76096   1.13407 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "10  -1.42424   0.59042   0.12331 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "\n",
       "   meth mushroom nicotine semer  vsa  \n",
       "id                                    \n",
       "1   CL0      CL0      CL2   CL0  CL0  \n",
       "2   CL3      CL0      CL4   CL0  CL0  \n",
       "3   CL0      CL1      CL0   CL0  CL0  \n",
       "4   CL0      CL0      CL2   CL0  CL0  \n",
       "5   CL0      CL2      CL2   CL0  CL0  \n",
       "6   CL0      CL0      CL6   CL0  CL0  \n",
       "7   CL0      CL0      CL6   CL0  CL0  \n",
       "8   CL0      CL0      CL0   CL0  CL0  \n",
       "9   CL0      CL0      CL6   CL0  CL0  \n",
       "10  CL0      CL0      CL6   CL0  CL0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The column names for our csv data.\n",
    "csv_columns = [\n",
    "    'id', 'age', 'gender', 'education', 'country', 'ethnicity', 'n_score',\n",
    "    'e_score', 'o_score', 'a_score', 'c_score', 'impulsive', 'ss', 'alcohol', \n",
    "    'amphet', 'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'coke', 'crack', \n",
    "    'ecstasy', 'heroin', 'ketamine', 'legal_h', 'lsd', 'meth', 'mushroom', \n",
    "    'nicotine', 'semer', 'vsa'\n",
    "] \n",
    "\n",
    "\n",
    "# Read the entire csv (no header), apply column names to the data, and use the 'id' column as an index. \n",
    "# Force String type on all records.\n",
    "\n",
    "data = pd.read_csv(clean_file, header=None, names=csv_columns, index_col='id', dtype=str)\n",
    "\n",
    "# Display the top 10 records.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe contains 3 types of data: demographic information, psychological personality scores, and drug-use survey answers.\n",
    "\n",
    "In this case, we are interested in observing the potential abuse of heroin. We define the presence of a risk of heroin abuse as having claimed to have used heroin within the last year.\n",
    "\n",
    "To have this reflect in our Dataframe, we map each response to a 0 (representing no presence of risk of abuse) or 1 (representing presence of a risk of abuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_map = {\n",
    "    \"CL0\" : 0, # Never Used\n",
    "    \"CL1\" : 0, # Used over a Decade Ago\n",
    "    \"CL2\" : 0, # Used in Last Decade\n",
    "    \"CL3\" : 1, # Used in Last Year\n",
    "    \"CL4\" : 1, # Used in Last Month\n",
    "    \"CL5\" : 1, # Used in Last Week\n",
    "    \"CL6\" : 1 # Used in Last Day\n",
    "}\n",
    "\n",
    "# If a given key is present in the risk_map, return its value. Otherwise, default to 0.\n",
    "def assess_risk(key):\n",
    "    if key in risk_map:\n",
    "        return risk_map[key]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Reassign the values of the 'heroin' column to their corresponding value in the risk_map.\n",
    "data['heroin'] = data['heroin'].apply(assess_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform any last data engineering tasks before running creating our train and test data sets. Below, we specify many lookup maps for the various categorical features. These maps are used to convert the numerical representation of the data to its contextual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>n_score</th>\n",
       "      <th>e_score</th>\n",
       "      <th>o_score</th>\n",
       "      <th>a_score</th>\n",
       "      <th>c_score</th>\n",
       "      <th>...</th>\n",
       "      <th>amphet</th>\n",
       "      <th>amyl</th>\n",
       "      <th>benzos</th>\n",
       "      <th>caff</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>choc</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35-44</td>\n",
       "      <td>f</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>UK</td>\n",
       "      <td>Mixed-White/Asian</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Week</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-34</td>\n",
       "      <td>m</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>0</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35-44</td>\n",
       "      <td>m</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Used in Last Year</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18-24</td>\n",
       "      <td>f</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Year</td>\n",
       "      <td>Used in Last Week</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>0</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>35-44</td>\n",
       "      <td>f</td>\n",
       "      <td>Doctorate degree</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Used in Last Year</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Decade</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>65+</td>\n",
       "      <td>f</td>\n",
       "      <td>Left school at 18 years</td>\n",
       "      <td>Canada</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>-0.30033</td>\n",
       "      <td>-1.55521</td>\n",
       "      <td>2.03972</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>45-54</td>\n",
       "      <td>m</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>USA</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>-1.09207</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>0.93949</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Used in Last Week</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>35-44</td>\n",
       "      <td>m</td>\n",
       "      <td>Left school at 16 years</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>-1.32828</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Month</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>35-44</td>\n",
       "      <td>f</td>\n",
       "      <td>Professional certificate/ diploma</td>\n",
       "      <td>Canada</td>\n",
       "      <td>White</td>\n",
       "      <td>0.62967</td>\n",
       "      <td>2.57309</td>\n",
       "      <td>-0.97631</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>1.13407</td>\n",
       "      <td>...</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55-64</td>\n",
       "      <td>m</td>\n",
       "      <td>Masters degree</td>\n",
       "      <td>UK</td>\n",
       "      <td>White</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>-1.42424</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.12331</td>\n",
       "      <td>...</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Used over a Decade Ago</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>0</td>\n",
       "      <td>Never Used</td>\n",
       "      <td>Used in Last Day</td>\n",
       "      <td>Never Used</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age gender                          education country  \\\n",
       "id                                                            \n",
       "1   35-44      f  Professional certificate/ diploma      UK   \n",
       "2   25-34      m                   Doctorate degree      UK   \n",
       "3   35-44      m  Professional certificate/ diploma      UK   \n",
       "4   18-24      f                     Masters degree      UK   \n",
       "5   35-44      f                   Doctorate degree      UK   \n",
       "6     65+      f            Left school at 18 years  Canada   \n",
       "7   45-54      m                     Masters degree     USA   \n",
       "8   35-44      m            Left school at 16 years      UK   \n",
       "9   35-44      f  Professional certificate/ diploma  Canada   \n",
       "10  55-64      m                     Masters degree      UK   \n",
       "\n",
       "            ethnicity   n_score   e_score   o_score   a_score   c_score  \\\n",
       "id                                                                        \n",
       "1   Mixed-White/Asian   0.31287  -0.57545  -0.58331  -0.91699  -0.00665   \n",
       "2               White  -0.67825   1.93886   1.43533   0.76096  -0.14277   \n",
       "3               White  -0.46725   0.80523  -0.84732  -1.62090  -1.01450   \n",
       "4               White  -0.14882  -0.80615  -0.01928   0.59042   0.58489   \n",
       "5               White   0.73545  -1.63340  -0.45174  -0.30172   1.30612   \n",
       "6               White  -0.67825  -0.30033  -1.55521   2.03972   1.63088   \n",
       "7               White  -0.46725  -1.09207  -0.45174  -0.30172   0.93949   \n",
       "8               White  -1.32828   1.93886  -0.84732  -0.30172   1.63088   \n",
       "9               White   0.62967   2.57309  -0.97631   0.76096   1.13407   \n",
       "10              White  -0.24649   0.00332  -1.42424   0.59042   0.12331   \n",
       "\n",
       "       ...                      amphet                    amyl  \\\n",
       "id     ...                                                       \n",
       "1      ...         Used in Last Decade              Never Used   \n",
       "2      ...         Used in Last Decade     Used in Last Decade   \n",
       "3      ...                  Never Used              Never Used   \n",
       "4      ...                  Never Used              Never Used   \n",
       "5      ...      Used over a Decade Ago  Used over a Decade Ago   \n",
       "6      ...                  Never Used              Never Used   \n",
       "7      ...                  Never Used              Never Used   \n",
       "8      ...                  Never Used              Never Used   \n",
       "9      ...                  Never Used              Never Used   \n",
       "10     ...      Used over a Decade Ago              Never Used   \n",
       "\n",
       "                    benzos               caff                cannabis  \\\n",
       "id                                                                      \n",
       "1      Used in Last Decade   Used in Last Day              Never Used   \n",
       "2               Never Used   Used in Last Day      Used in Last Month   \n",
       "3               Never Used   Used in Last Day       Used in Last Year   \n",
       "4        Used in Last Year  Used in Last Week     Used in Last Decade   \n",
       "5               Never Used   Used in Last Day       Used in Last Year   \n",
       "6               Never Used   Used in Last Day              Never Used   \n",
       "7               Never Used   Used in Last Day  Used over a Decade Ago   \n",
       "8               Never Used   Used in Last Day              Never Used   \n",
       "9               Never Used   Used in Last Day              Never Used   \n",
       "10  Used over a Decade Ago   Used in Last Day  Used over a Decade Ago   \n",
       "\n",
       "                  choc heroin             ketamine             nicotine  \\\n",
       "id                                                                        \n",
       "1    Used in Last Week      0           Never Used  Used in Last Decade   \n",
       "2     Used in Last Day      0  Used in Last Decade   Used in Last Month   \n",
       "3   Used in Last Month      0           Never Used           Never Used   \n",
       "4   Used in Last Month      0  Used in Last Decade  Used in Last Decade   \n",
       "5     Used in Last Day      0           Never Used  Used in Last Decade   \n",
       "6   Used in Last Month      0           Never Used     Used in Last Day   \n",
       "7    Used in Last Week      0           Never Used     Used in Last Day   \n",
       "8   Used in Last Month      0           Never Used           Never Used   \n",
       "9     Used in Last Day      0           Never Used     Used in Last Day   \n",
       "10    Used in Last Day      0           Never Used     Used in Last Day   \n",
       "\n",
       "           vsa  \n",
       "id              \n",
       "1   Never Used  \n",
       "2   Never Used  \n",
       "3   Never Used  \n",
       "4   Never Used  \n",
       "5   Never Used  \n",
       "6   Never Used  \n",
       "7   Never Used  \n",
       "8   Never Used  \n",
       "9   Never Used  \n",
       "10  Never Used  \n",
       "\n",
       "[10 rows x 23 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function that takes a lookup map and a key and returns the value of the key, or None if the key is not present.\n",
    "def lookup(lookup_map, key):\n",
    "    if key in lookup_map:\n",
    "        return lookup_map[key]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "age_map = {\n",
    "    '-0.95197':'18-24',\n",
    "    '-0.07854':'25-34',\n",
    "    '0.49788':'35-44',\n",
    "    '1.09449':'45-54',\n",
    "    '1.82213':'55-64',\n",
    "    '2.59171':'65+'\n",
    "}\n",
    "\n",
    "gender_map = {\n",
    "    '0.48246': 'f',\n",
    "    '-0.48246': 'm'\n",
    "}\n",
    "\n",
    "education_map = {\n",
    "    '-2.43591': 'Left school before 16 years',\n",
    "    '-1.73790': 'Left school at 16 years',\n",
    "    '-1.43719': 'Left school at 17 years',\n",
    "    '-1.22751': 'Left school at 18 years',\n",
    "    '-0.61113': 'Some college or university, no certificate or degree',\n",
    "    '-0.05921': 'Professional certificate/ diploma',\n",
    "    '0.45468': 'University degree',\n",
    "    '1.16365': 'Masters degree',\n",
    "    '1.98437': 'Doctorate degree'\n",
    "}\n",
    "\n",
    "country_map = {\n",
    "    '-0.09765': 'Australia',\n",
    "    '0.24923': 'Canada',\n",
    "    '-0.46841': 'New Zealand',\n",
    "    '-0.28519': 'Other',\n",
    "    '0.21128': 'Republic of Ireland',\n",
    "    '0.96082': 'UK',\n",
    "    '-0.57009': 'USA'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    '-0.50212': 'Asian',\n",
    "    '-1.10702': 'Black',\n",
    "    '1.90725': 'Mixed-Black/Asian',\n",
    "    '0.12600': 'Mixed-White/Asian',\n",
    "    '-0.22166': 'Mixed-White/Black',\n",
    "    '0.11440': 'Other',\n",
    "    '-0.31685': 'White'\n",
    "}\n",
    "\n",
    "consumption_map = {\n",
    "    \"CL0\": \"Never Used\",\n",
    "    \"CL1\": \"Used over a Decade Ago\",\n",
    "    \"CL2\": \"Used in Last Decade\",\n",
    "    \"CL3\": \"Used in Last Year\",\n",
    "    \"CL4\": \"Used in Last Month\",\n",
    "    \"CL5\": \"Used in Last Week\",\n",
    "    \"CL6\": \"Used in Last Day\"\n",
    "}\n",
    "\n",
    "data['age'] = data['age'].apply(lambda x: lookup(age_map, x))\n",
    "data['gender'] = data['gender'].apply(lambda x: lookup(gender_map, x))\n",
    "data['education'] = data['education'].apply(lambda x: lookup(education_map, x))\n",
    "data['country'] = data['country'].apply(lambda x: lookup(country_map, x))\n",
    "data['ethnicity'] = data['ethnicity'].apply(lambda x: lookup(ethnicity_map, x))\n",
    "data['alcohol'] = data['alcohol'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['amphet'] = data['amphet'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['amyl'] = data['amyl'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['benzos'] = data['benzos'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['caff'] = data['caff'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['cannabis'] = data['cannabis'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['choc'] = data['choc'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['ketamine'] = data['ketamine'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['nicotine'] = data['nicotine'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['vsa'] = data['vsa'].apply(lambda x: lookup(consumption_map, x))\n",
    "data.drop('coke', axis=1, inplace=True)\n",
    "data.drop('crack', axis=1, inplace=True)\n",
    "data.drop('ecstasy', axis=1, inplace=True)\n",
    "data.drop('legal_h', axis=1, inplace=True)\n",
    "data.drop('lsd', axis=1, inplace=True)\n",
    "data.drop('meth', axis=1, inplace=True)\n",
    "data.drop('semer', axis=1, inplace=True)\n",
    "data.drop('mushroom', axis=1, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our 'heroin' column properly labeled, and our data fields converted to something a little more readable, we can now do some analysis. Below are some aggregations that allow us to see how the distribution of at-risk people across various demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age    heroin\n",
      "18-24  0         573\n",
      "       1          70\n",
      "25-34  0         449\n",
      "       1          32\n",
      "35-44  0         346\n",
      "       1          10\n",
      "45-54  0         288\n",
      "       1           6\n",
      "55-64  0          93\n",
      "65+    0          18\n",
      "Name: age, dtype: int64 \n",
      "\n",
      "gender  heroin\n",
      "f       0         909\n",
      "        1          33\n",
      "m       0         858\n",
      "        1          85\n",
      "Name: gender, dtype: int64 \n",
      "\n",
      "country              heroin\n",
      "Australia            0           51\n",
      "                     1            3\n",
      "Canada               0           76\n",
      "                     1           11\n",
      "New Zealand          0            5\n",
      "Other                0          114\n",
      "                     1            4\n",
      "Republic of Ireland  0           18\n",
      "                     1            2\n",
      "UK                   0         1035\n",
      "                     1            9\n",
      "USA                  0          468\n",
      "                     1           89\n",
      "Name: country, dtype: int64 \n",
      "\n",
      "ethnicity          heroin\n",
      "Asian              0           26\n",
      "Black              0           32\n",
      "                   1            1\n",
      "Mixed-Black/Asian  0            3\n",
      "Mixed-White/Asian  0           16\n",
      "                   1            4\n",
      "Mixed-White/Black  0           17\n",
      "                   1            3\n",
      "Other              0           57\n",
      "                   1            6\n",
      "White              0         1616\n",
      "                   1          104\n",
      "Name: ethnicity, dtype: int64 \n",
      "\n",
      "education                                             heroin\n",
      "Doctorate degree                                      0          85\n",
      "                                                      1           4\n",
      "Left school at 16 years                               0          94\n",
      "                                                      1           5\n",
      "Left school at 17 years                               0          27\n",
      "                                                      1           3\n",
      "Left school at 18 years                               0          88\n",
      "                                                      1          12\n",
      "Left school before 16 years                           0          25\n",
      "                                                      1           3\n",
      "Masters degree                                        0         276\n",
      "                                                      1           7\n",
      "Professional certificate/ diploma                     0         260\n",
      "                                                      1          10\n",
      "Some college or university, no certificate or degree  0         454\n",
      "                                                      1          52\n",
      "University degree                                     0         458\n",
      "                                                      1          22\n",
      "Name: education, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "age = data.groupby(['age', 'heroin'])['age'].count()\n",
    "gender = data.groupby(['gender', 'heroin'])['gender'].count()\n",
    "country = data.groupby(['country', 'heroin'])['country'].count()\n",
    "ethnicity = data.groupby(['ethnicity', 'heroin'])['ethnicity'].count()\n",
    "education = data.groupby(['education', 'heroin'])['education'].count()\n",
    "\n",
    "print(age, \"\\n\")\n",
    "print(gender, \"\\n\")\n",
    "print(country, \"\\n\")\n",
    "print(ethnicity, \"\\n\")\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By making use of matplotlib, we are able to take data filters and aggregations and create visualiations to provide a better understanding of what the populations look like. In this case, we generate some bar graphs. For each graph, we declare the number of groups, perform some filtering to obtain the values of the groups we desire, assign bars to each group, and finally plot the graph out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe0717ac320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_groups = 2\n",
    "\n",
    "# Collect Men Data\n",
    "men_at_risk = data['gender'][data['gender'] == 'm'][data['heroin'] == 1].count()\n",
    "men_no_risk = data['gender'][data['gender'] == 'm'][data['heroin'] == 0].count()\n",
    "men = (men_at_risk, men_no_risk)\n",
    "\n",
    "# Collect Women Data\n",
    "women_at_risk = data['gender'][data['gender'] == 'f'][data['heroin'] == 1].count()\n",
    "women_no_risk = data['gender'][data['gender'] == 'f'][data['heroin'] == 0].count()\n",
    "women = (women_at_risk, women_no_risk)\n",
    "\n",
    "fig, gender_plot = plt.subplots()\n",
    "\n",
    "# Assign Bars\n",
    "gender_ind = np.arange(gender_groups)\n",
    "width = 0.35\n",
    "\n",
    "men_bar = gender_plot.bar(gender_ind, men, width, color='b')\n",
    "women_bar = gender_plot.bar(gender_ind + width, women, width, color='r')\n",
    "\n",
    "# Plot Configuration\n",
    "gender_plot.set_ylabel('Population')\n",
    "gender_plot.set_title('Population Count By Risk and Gender')\n",
    "gender_plot.set_xticks(gender_ind + width / 2)\n",
    "gender_plot.set_xticklabels(('At Risk', 'No Risk'))\n",
    "gender_plot.legend((men_bar[0], women_bar[0]), ('Men', 'Women'))\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data up into train and test data. This split data will be used to train and validate our Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_count: 1319\n",
      "testing_count: 566\n",
      "train_positives: 79\n",
      "train_negatives: 1240\n",
      "test_positives: 39\n",
      "test_negatives: 527\n"
     ]
    }
   ],
   "source": [
    "train_file_name = 'drug.data'\n",
    "train_file = \"%s/%s\" % (data_directory, train_file_name)\n",
    "\n",
    "test_file_name = 'drug.test'\n",
    "test_file = \"%s/%s\" % (data_directory, test_file_name)\n",
    "\n",
    "# Randomly Sample 70% of the Data to serve as Training Data.\n",
    "train = data.sample(n = int(len(data.index) * .7))\n",
    "\n",
    "# Remove all samples selected as Training Data to get the remaining Test Data.\n",
    "test = data.drop(train.index)\n",
    "\n",
    "print(\"train_count: %s\\ntesting_count: %s\" % (len(train.index), len(test.index)))\n",
    "\n",
    "# Calculate number of Positive and Negative events for Training and Test Data.\n",
    "train_positives = len(train[train['heroin'].isin([1])].index)\n",
    "train_negatives = len(train.index) - train_positives\n",
    "test_positives = len(test[test['heroin'].isin([1])].index)\n",
    "test_negatives = len(test.index) - test_positives\n",
    "\n",
    "print(\"train_positives: %s\\ntrain_negatives: %s\" % (train_positives, train_negatives))\n",
    "print(\"test_positives: %s\\ntest_negatives: %s\" % (test_positives, test_negatives))\n",
    "\n",
    "train.to_csv(train_file, header=False)\n",
    "test.to_csv(test_file, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Tensorflow Model  \n",
    "With all of our data engineering out of the way, we are ready to set up our tensors and train a Tensorflow model to perform predictions.\n",
    "\n",
    "Similar to when we first downloaded and engineered our data, we specify some values that will be used to manage our model, including its location and some configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU usage as we do not need it for our model.\n",
    "run_config = tf.estimator.RunConfig().replace(\n",
    "  session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "# Directory where the model will be stored.\n",
    "model_directory = '/tmp/drug_model'\n",
    "\n",
    "# Specify number of forward and back passes over the whole training set.\n",
    "train_epochs = 40\n",
    "\n",
    "# Specify number of passes prior to each evaluation.\n",
    "epochs_per_eval = 2\n",
    "\n",
    "# Specify number of training instances to pass over at a time.\n",
    "batch_size = 20\n",
    "\n",
    "# Specify number of hidden layers and nodes in the Deep Neural Network Model.\n",
    "hidden_units = [100, 75, 50, 25]\n",
    "\n",
    "# Specify number of examples for each set of data.\n",
    "num_examples = {\n",
    "    'train': 1319,\n",
    "    'validation': 566,\n",
    "}\n",
    "\n",
    "# Specify remaining columns after our data clean up process.\n",
    "train_columns = [\n",
    "    'id', 'age', 'gender', 'education', 'country', 'ethnicity', 'n_score', 'e_score',\n",
    "    'o_score', 'a_score', 'c_score', 'impulsive', 'ss', 'alcohol', 'amphet',\n",
    "    'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'heroin', 'ketamine', 'nicotine', 'vsa']\n",
    "\n",
    "# Specify default vlaues for missing values for each column.\n",
    "train_column_defaults = [\n",
    "    [''], [''], [''], [''], [''], [''], [0.0], [0.0], \n",
    "    [0.0], [0.0], [0.0], [0.0], [0.0], [''], [''], \n",
    "    [''], [''], [''], [''], [''], [''], [''], [''], ['']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some of the model configuration out of the way, we can begin setting up our Tensors. For a long time, I couldn't wrap my head around what the word \"Tensor\" really meant, but now I have realized it's just a really fancy word for a Feature that has been specified to pass through a Tensorflow model. A little unnecessary, but hey, I didn't write the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features to pass into the Linear Model.\n",
    "age = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'age', [\n",
    "      '18-24','25-34','35-44','45-54','55-64','65+'])\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'gender', [\n",
    "      'f', 'm'])\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'education', [\n",
    "      'Left school before 16 years', 'Left school at 16 years',\n",
    "      'Left school at 17 years', 'Left school at 18 years',\n",
    "      'Some college or university, no certificate or degree',\n",
    "      'Professional certificate/ diploma', 'University degree',\n",
    "      'Masters degree', 'Doctorate degree'])\n",
    "\n",
    "country = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'country', [\n",
    "      'Australia', 'Canada', 'New Zealand', 'Other', \n",
    "      'Republic of Ireland', 'UK', 'USA'])\n",
    "\n",
    "ethnicity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'ethnicity', [\n",
    "      'Asian', 'Black', 'Mixed-Black/Asian', 'Mixed-White/Asian', \n",
    "      'Mixed-White/Black', 'Other', 'White'])\n",
    "\n",
    "alcohol = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'alcohol', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "amphet = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'amphet', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "amyl = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'amyl', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "benzos = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'benzos', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "caff = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'caff', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "cannabis = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'cannabis', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "choc = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'choc', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "ketamine = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'ketamine', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "vsa = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'vsa', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "base_columns = [\n",
    "  age, gender, education, country, ethnicity, alcohol, amphet, \n",
    "  amyl, benzos, caff, cannabis, choc, ketamine]\n",
    "\n",
    "# Crossed Columns allow us to create cross products of Categorical Feature Columns.\n",
    "crossed_columns = [\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['age', 'education'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['age', 'education', 'ethnicity'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['education', 'country'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'education'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'country'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'country', 'education'], hash_bucket_size=1000)]\n",
    "\n",
    "# Numerical Features to be passed into the Deep Neural Network Model.\n",
    "n_score = tf.feature_column.numeric_column('n_score')\n",
    "\n",
    "e_score = tf.feature_column.numeric_column('e_score')\n",
    "\n",
    "o_score = tf.feature_column.numeric_column('o_score')\n",
    "\n",
    "a_score = tf.feature_column.numeric_column('a_score')\n",
    "\n",
    "c_score = tf.feature_column.numeric_column('c_score')\n",
    "\n",
    "impulsive = tf.feature_column.numeric_column('impulsive')\n",
    "\n",
    "ss = tf.feature_column.numeric_column('ss')\n",
    "\n",
    "# The final input for our Linear Model.\n",
    "wide_columns = base_columns + crossed_columns\n",
    "\n",
    "# The final input for our Deep Neural Network Model.\n",
    "deep_columns = [\n",
    "  n_score, e_score, o_score, a_score, c_score, impulsive, ss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our configuration set and our columns specified, we are ready to create our TensorFlow model object. We use the [`DNNLinearCombinedClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier) in order to perform the combination of Wide and Deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/drug_model', '_tf_random_seed': 1, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_session_config': device_count {\n",
      "  key: \"GPU\"\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_directory,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step prior to running our TensorFlow Learning Algorithm is to create an input function. This input function will handle everything from loading the data to be used, shuffling it after every evaluation, and extracting the features and labels from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input function for training and evaluating.\n",
    "def input_fn(run_type, data_file, shuffle, num_epochs, batch_size):\n",
    "  \n",
    "  # Check to see if the file specified exists.\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found.' % data_file)\n",
    "\n",
    "  # Extract Text Data\n",
    "  dataset = tf.contrib.data.TextLineDataset(data_file)\n",
    "\n",
    "  if run_type == shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=num_examples[run_type])\n",
    "\n",
    "  # Convert our text data into features and labels.\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, record_defaults=train_column_defaults)\n",
    "    features = dict(zip(train_columns, columns))\n",
    "    labels = features.pop('heroin')\n",
    "    return features, tf.equal(labels, '1')\n",
    "    \n",
    "  dataset = dataset.map(parse_csv)\n",
    "\n",
    "  # Repeat the dataset for the number of epochs we are running.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "\n",
    "  # Create a one-use iterator to enable TensorFlow to pass through the data.\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but certainly not least, we wipe out our model directory and start training a brand new model using our input functions and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 16.3888, step = 1\n",
      "INFO:tensorflow:global_step/sec: 168.475\n",
      "INFO:tensorflow:loss = 6.82284, step = 101 (0.596 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 132 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 4.56124.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:32:49\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-132\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:32:50\n",
      "INFO:tensorflow:Saving dict for global step 132: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.751204, auc_precision_recall = 0.140331, average_loss = 0.270573, global_step = 132, label/mean = 0.0689046, loss = 5.28084, prediction/mean = 0.1508\n",
      "Results at epoch 2\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.751204\n",
      "auc_precision_recall: 0.140331\n",
      "average_loss: 0.270573\n",
      "global_step: 132\n",
      "label/mean: 0.0689046\n",
      "loss: 5.28084\n",
      "prediction/mean: 0.1508\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-132\n",
      "INFO:tensorflow:Saving checkpoints for 133 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.15963, step = 133\n",
      "INFO:tensorflow:global_step/sec: 165.682\n",
      "INFO:tensorflow:loss = 5.87394, step = 233 (0.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 264 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.8514.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:32:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-264\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:32:59\n",
      "INFO:tensorflow:Saving dict for global step 264: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.772199, auc_precision_recall = 0.15259, average_loss = 0.23486, global_step = 264, label/mean = 0.0689046, loss = 4.58382, prediction/mean = 0.094192\n",
      "Results at epoch 4\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.772199\n",
      "auc_precision_recall: 0.15259\n",
      "average_loss: 0.23486\n",
      "global_step: 264\n",
      "label/mean: 0.0689046\n",
      "loss: 4.58382\n",
      "prediction/mean: 0.094192\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-264\n",
      "INFO:tensorflow:Saving checkpoints for 265 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.80281, step = 265\n",
      "INFO:tensorflow:global_step/sec: 169.918\n",
      "INFO:tensorflow:loss = 5.63313, step = 365 (0.591 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 396 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.65727.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-396\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:08\n",
      "INFO:tensorflow:Saving dict for global step 396: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.795042, auc_precision_recall = 0.166948, average_loss = 0.225168, global_step = 396, label/mean = 0.0689046, loss = 4.39466, prediction/mean = 0.0771087\n",
      "Results at epoch 6\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.795042\n",
      "auc_precision_recall: 0.166948\n",
      "average_loss: 0.225168\n",
      "global_step: 396\n",
      "label/mean: 0.0689046\n",
      "loss: 4.39466\n",
      "prediction/mean: 0.0771087\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-396\n",
      "INFO:tensorflow:Saving checkpoints for 397 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.40535, step = 397\n",
      "INFO:tensorflow:global_step/sec: 163.048\n",
      "INFO:tensorflow:loss = 5.48693, step = 497 (0.616 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 528 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.5533.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-528\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:16\n",
      "INFO:tensorflow:Saving dict for global step 528: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.804214, auc_precision_recall = 0.177935, average_loss = 0.220252, global_step = 528, label/mean = 0.0689046, loss = 4.29871, prediction/mean = 0.0701836\n",
      "Results at epoch 8\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.804214\n",
      "auc_precision_recall: 0.177935\n",
      "average_loss: 0.220252\n",
      "global_step: 528\n",
      "label/mean: 0.0689046\n",
      "loss: 4.29871\n",
      "prediction/mean: 0.0701836\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-528\n",
      "INFO:tensorflow:Saving checkpoints for 529 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.2377, step = 529\n",
      "INFO:tensorflow:global_step/sec: 159.617\n",
      "INFO:tensorflow:loss = 5.37025, step = 629 (0.629 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 660 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.47793.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-660\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:25\n",
      "INFO:tensorflow:Saving dict for global step 660: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.809444, auc_precision_recall = 0.183559, average_loss = 0.217039, global_step = 660, label/mean = 0.0689046, loss = 4.23601, prediction/mean = 0.0666932\n",
      "Results at epoch 10\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.809444\n",
      "auc_precision_recall: 0.183559\n",
      "average_loss: 0.217039\n",
      "global_step: 660\n",
      "label/mean: 0.0689046\n",
      "loss: 4.23601\n",
      "prediction/mean: 0.0666932\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-660\n",
      "INFO:tensorflow:Saving checkpoints for 661 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.14842, step = 661\n",
      "INFO:tensorflow:global_step/sec: 167.87\n",
      "INFO:tensorflow:loss = 5.27057, step = 761 (0.598 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 792 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.41874.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-792\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:34\n",
      "INFO:tensorflow:Saving dict for global step 792: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.816961, auc_precision_recall = 0.189743, average_loss = 0.214669, global_step = 792, label/mean = 0.0689046, loss = 4.18974, prediction/mean = 0.064713\n",
      "Results at epoch 12\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.816961\n",
      "auc_precision_recall: 0.189743\n",
      "average_loss: 0.214669\n",
      "global_step: 792\n",
      "label/mean: 0.0689046\n",
      "loss: 4.18974\n",
      "prediction/mean: 0.064713\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-792\n",
      "INFO:tensorflow:Saving checkpoints for 793 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.09362, step = 793\n",
      "INFO:tensorflow:global_step/sec: 164.548\n",
      "INFO:tensorflow:loss = 5.18331, step = 893 (0.610 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 924 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.37016.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-924\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:42\n",
      "INFO:tensorflow:Saving dict for global step 924: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.81954, auc_precision_recall = 0.193182, average_loss = 0.212805, global_step = 924, label/mean = 0.0689046, loss = 4.15336, prediction/mean = 0.0634263\n",
      "Results at epoch 14\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.81954\n",
      "auc_precision_recall: 0.193182\n",
      "average_loss: 0.212805\n",
      "global_step: 924\n",
      "label/mean: 0.0689046\n",
      "loss: 4.15336\n",
      "prediction/mean: 0.0634263\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-924\n",
      "INFO:tensorflow:Saving checkpoints for 925 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.05601, step = 925\n",
      "INFO:tensorflow:global_step/sec: 166.661\n",
      "INFO:tensorflow:loss = 5.10439, step = 1025 (0.602 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1056 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.32747.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1056\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:33:51\n",
      "INFO:tensorflow:Saving dict for global step 1056: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.823359, auc_precision_recall = 0.198326, average_loss = 0.211212, global_step = 1056, label/mean = 0.0689046, loss = 4.12227, prediction/mean = 0.0626388\n",
      "Results at epoch 16\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.823359\n",
      "auc_precision_recall: 0.198326\n",
      "average_loss: 0.211212\n",
      "global_step: 1056\n",
      "label/mean: 0.0689046\n",
      "loss: 4.12227\n",
      "prediction/mean: 0.0626388\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1056\n",
      "INFO:tensorflow:Saving checkpoints for 1057 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.03031, step = 1057\n",
      "INFO:tensorflow:global_step/sec: 167.686\n",
      "INFO:tensorflow:loss = 5.03725, step = 1157 (0.599 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1188 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.28924.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:33:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1188\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:00\n",
      "INFO:tensorflow:Saving dict for global step 1188: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.827227, auc_precision_recall = 0.202906, average_loss = 0.209988, global_step = 1188, label/mean = 0.0689046, loss = 4.09838, prediction/mean = 0.0617222\n",
      "Results at epoch 18\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.827227\n",
      "auc_precision_recall: 0.202906\n",
      "average_loss: 0.209988\n",
      "global_step: 1188\n",
      "label/mean: 0.0689046\n",
      "loss: 4.09838\n",
      "prediction/mean: 0.0617222\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1188\n",
      "INFO:tensorflow:Saving checkpoints for 1189 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.00516, step = 1189\n",
      "INFO:tensorflow:global_step/sec: 165.939\n",
      "INFO:tensorflow:loss = 4.97364, step = 1289 (0.605 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1320 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.25378.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1320\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:08\n",
      "INFO:tensorflow:Saving dict for global step 1320: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.827251, auc_precision_recall = 0.203203, average_loss = 0.208936, global_step = 1320, label/mean = 0.0689046, loss = 4.07785, prediction/mean = 0.0610649\n",
      "Results at epoch 20\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.827251\n",
      "auc_precision_recall: 0.203203\n",
      "average_loss: 0.208936\n",
      "global_step: 1320\n",
      "label/mean: 0.0689046\n",
      "loss: 4.07785\n",
      "prediction/mean: 0.0610649\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1320\n",
      "INFO:tensorflow:Saving checkpoints for 1321 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.986271, step = 1321\n",
      "INFO:tensorflow:global_step/sec: 165.64\n",
      "INFO:tensorflow:loss = 4.91375, step = 1421 (0.607 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1452 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.22045.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1452\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:17\n",
      "INFO:tensorflow:Saving dict for global step 1452: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.829003, auc_precision_recall = 0.207933, average_loss = 0.208045, global_step = 1452, label/mean = 0.0689046, loss = 4.06047, prediction/mean = 0.06052\n",
      "Results at epoch 22\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.829003\n",
      "auc_precision_recall: 0.207933\n",
      "average_loss: 0.208045\n",
      "global_step: 1452\n",
      "label/mean: 0.0689046\n",
      "loss: 4.06047\n",
      "prediction/mean: 0.06052\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1452\n",
      "INFO:tensorflow:Saving checkpoints for 1453 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.970965, step = 1453\n",
      "INFO:tensorflow:global_step/sec: 162.82\n",
      "INFO:tensorflow:loss = 4.85883, step = 1553 (0.617 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1584 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.18845.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1584\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:26\n",
      "INFO:tensorflow:Saving dict for global step 1584: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.830341, auc_precision_recall = 0.207863, average_loss = 0.207277, global_step = 1584, label/mean = 0.0689046, loss = 4.04547, prediction/mean = 0.059943\n",
      "Results at epoch 24\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.830341\n",
      "auc_precision_recall: 0.207863\n",
      "average_loss: 0.207277\n",
      "global_step: 1584\n",
      "label/mean: 0.0689046\n",
      "loss: 4.04547\n",
      "prediction/mean: 0.059943\n",
      "Parsing /tmp/drug_data/drug.data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1584\n",
      "INFO:tensorflow:Saving checkpoints for 1585 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.956283, step = 1585\n",
      "INFO:tensorflow:global_step/sec: 167.268\n",
      "INFO:tensorflow:loss = 4.80798, step = 1685 (0.600 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1716 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.15852.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1716\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:34\n",
      "INFO:tensorflow:Saving dict for global step 1716: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.831703, auc_precision_recall = 0.209329, average_loss = 0.206593, global_step = 1716, label/mean = 0.0689046, loss = 4.03213, prediction/mean = 0.0595087\n",
      "Results at epoch 26\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.831703\n",
      "auc_precision_recall: 0.209329\n",
      "average_loss: 0.206593\n",
      "global_step: 1716\n",
      "label/mean: 0.0689046\n",
      "loss: 4.03213\n",
      "prediction/mean: 0.0595087\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1716\n",
      "INFO:tensorflow:Saving checkpoints for 1717 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.945213, step = 1717\n",
      "INFO:tensorflow:global_step/sec: 163.624\n",
      "INFO:tensorflow:loss = 4.76097, step = 1817 (0.614 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1848 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.13067.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1848\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:43\n",
      "INFO:tensorflow:Saving dict for global step 1848: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.831947, auc_precision_recall = 0.208029, average_loss = 0.205976, global_step = 1848, label/mean = 0.0689046, loss = 4.02009, prediction/mean = 0.0591004\n",
      "Results at epoch 28\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.831947\n",
      "auc_precision_recall: 0.208029\n",
      "average_loss: 0.205976\n",
      "global_step: 1848\n",
      "label/mean: 0.0689046\n",
      "loss: 4.02009\n",
      "prediction/mean: 0.0591004\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1848\n",
      "INFO:tensorflow:Saving checkpoints for 1849 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.93512, step = 1849\n",
      "INFO:tensorflow:global_step/sec: 166.384\n",
      "INFO:tensorflow:loss = 4.7161, step = 1949 (0.604 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1980 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.10457.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1980\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:34:51\n",
      "INFO:tensorflow:Saving dict for global step 1980: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.83326, auc_precision_recall = 0.209908, average_loss = 0.205401, global_step = 1980, label/mean = 0.0689046, loss = 4.00887, prediction/mean = 0.0587926\n",
      "Results at epoch 30\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.83326\n",
      "auc_precision_recall: 0.209908\n",
      "average_loss: 0.205401\n",
      "global_step: 1980\n",
      "label/mean: 0.0689046\n",
      "loss: 4.00887\n",
      "prediction/mean: 0.0587926\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-1980\n",
      "INFO:tensorflow:Saving checkpoints for 1981 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.927469, step = 1981\n",
      "INFO:tensorflow:global_step/sec: 165.876\n",
      "INFO:tensorflow:loss = 4.67351, step = 2081 (0.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2112 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.07972.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:34:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2112\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:35:00\n",
      "INFO:tensorflow:Saving dict for global step 2112: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.83455, auc_precision_recall = 0.2098, average_loss = 0.204869, global_step = 2112, label/mean = 0.0689046, loss = 3.99847, prediction/mean = 0.0585256\n",
      "Results at epoch 32\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.83455\n",
      "auc_precision_recall: 0.2098\n",
      "average_loss: 0.204869\n",
      "global_step: 2112\n",
      "label/mean: 0.0689046\n",
      "loss: 3.99847\n",
      "prediction/mean: 0.0585256\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2112\n",
      "INFO:tensorflow:Saving checkpoints for 2113 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.920781, step = 2113\n",
      "INFO:tensorflow:global_step/sec: 164.763\n",
      "INFO:tensorflow:loss = 4.6322, step = 2213 (0.610 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2244 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.05588.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:35:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2244\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:35:09\n",
      "INFO:tensorflow:Saving dict for global step 2244: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.836253, auc_precision_recall = 0.215438, average_loss = 0.20437, global_step = 2244, label/mean = 0.0689046, loss = 3.98873, prediction/mean = 0.0582498\n",
      "Results at epoch 34\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.836253\n",
      "auc_precision_recall: 0.215438\n",
      "average_loss: 0.20437\n",
      "global_step: 2244\n",
      "label/mean: 0.0689046\n",
      "loss: 3.98873\n",
      "prediction/mean: 0.0582498\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2244\n",
      "INFO:tensorflow:Saving checkpoints for 2245 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.914331, step = 2245\n",
      "INFO:tensorflow:global_step/sec: 165.67\n",
      "INFO:tensorflow:loss = 4.59352, step = 2345 (0.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2376 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.03309.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:35:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2376\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:35:18\n",
      "INFO:tensorflow:Saving dict for global step 2376: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.83888, auc_precision_recall = 0.216729, average_loss = 0.20392, global_step = 2376, label/mean = 0.0689046, loss = 3.97996, prediction/mean = 0.0579732\n",
      "Results at epoch 36\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.83888\n",
      "auc_precision_recall: 0.216729\n",
      "average_loss: 0.20392\n",
      "global_step: 2376\n",
      "label/mean: 0.0689046\n",
      "loss: 3.97996\n",
      "prediction/mean: 0.0579732\n",
      "Parsing /tmp/drug_data/drug.data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2376\n",
      "INFO:tensorflow:Saving checkpoints for 2377 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.908314, step = 2377\n",
      "INFO:tensorflow:global_step/sec: 165.907\n",
      "INFO:tensorflow:loss = 4.55587, step = 2477 (0.606 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2508 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.01132.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:35:26\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2508\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:35:26\n",
      "INFO:tensorflow:Saving dict for global step 2508: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.840267, auc_precision_recall = 0.216222, average_loss = 0.203488, global_step = 2508, label/mean = 0.0689046, loss = 3.97153, prediction/mean = 0.0577534\n",
      "Results at epoch 38\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.840267\n",
      "auc_precision_recall: 0.216222\n",
      "average_loss: 0.203488\n",
      "global_step: 2508\n",
      "label/mean: 0.0689046\n",
      "loss: 3.97153\n",
      "prediction/mean: 0.0577534\n",
      "Parsing /tmp/drug_data/drug.data\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2508\n",
      "INFO:tensorflow:Saving checkpoints for 2509 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.903612, step = 2509\n",
      "INFO:tensorflow:global_step/sec: 166\n",
      "INFO:tensorflow:loss = 4.52102, step = 2609 (0.605 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2640 into /tmp/drug_model/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.99015.\n",
      "Parsing /tmp/drug_data/drug.test\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "WARNING:tensorflow:Casting <dtype: 'float32'> labels to bool.\n",
      "INFO:tensorflow:Starting evaluation at 2018-01-29-23:35:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/drug_model/model.ckpt-2640\n",
      "INFO:tensorflow:Finished evaluation at 2018-01-29-23:35:35\n",
      "INFO:tensorflow:Saving dict for global step 2640: accuracy = 0.931095, accuracy_baseline = 0.931095, auc = 0.840948, auc_precision_recall = 0.218762, average_loss = 0.203088, global_step = 2640, label/mean = 0.0689046, loss = 3.96372, prediction/mean = 0.0574891\n",
      "Results at epoch 40\n",
      "------------------------------------------------------------\n",
      "accuracy: 0.931095\n",
      "accuracy_baseline: 0.931095\n",
      "auc: 0.840948\n",
      "auc_precision_recall: 0.218762\n",
      "average_loss: 0.203088\n",
      "global_step: 2640\n",
      "label/mean: 0.0689046\n",
      "loss: 3.96372\n",
      "prediction/mean: 0.0574891\n"
     ]
    }
   ],
   "source": [
    "# Clean up the model directory if present\n",
    "shutil.rmtree(model_directory, ignore_errors=True)\n",
    "\n",
    "# Train and evaluate the model every `epochs_per_eval` epochs.\n",
    "for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        run_type = 'train',\n",
    "        data_file = train_file, \n",
    "        shuffle = True,\n",
    "        num_epochs = epochs_per_eval, \n",
    "        batch_size = batch_size))\n",
    "\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        run_type = 'test',\n",
    "        data_file = test_file, \n",
    "        shuffle = False,\n",
    "        num_epochs = 1,\n",
    "        batch_size = batch_size))\n",
    "\n",
    "    # Display evaluation metrics.\n",
    "    print('Results at epoch', (n + 1) * epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
