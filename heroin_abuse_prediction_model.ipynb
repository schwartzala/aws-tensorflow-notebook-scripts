{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# At-Risk Heroin-Abuse Prediction Model Using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook contains Python code to identify a human being as a potential abuser of heroin. Using the UCI dataset for drug consumption located, we are able to build a Wide-Deep Learning Model that combines the benefits of Linear Models with Sparse and Crossed Features and Deep Neural Networs.  \n",
    "\n",
    "The Drug Consumption data set with documentation can be located here: https://archive.ics.uci.edu/ml/datasets/Drug+consumption+(quantified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import all of the required packages for Tensorflow and Data Engineering:  \n",
    "* [tensorflow](https://www.tensorflow.org)\n",
    "* [numpy](http://www.numpy.org)\n",
    "* [pandas](http://pandas.pydata.org)\n",
    "* [matplotlib](https://matplotlib.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "from six.moves import urllib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Clean Up the Data  \n",
    "First, we set some variables to manage where our data is located and where we want it to live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00373'\n",
    "file_name = 'drug_consumption.data'\n",
    "file_url = '%s/%s' % (url, file_name)\n",
    "data_directory = '/tmp/drug_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we download the data, perform some cleaning on it, and save it to our data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If directory does not exist, make the directory.\n",
    "if not tf.gfile.Exists(data_directory):\n",
    "    tf.gfile.MkDir(data_directory)\n",
    "\n",
    "# Create a temporary file to hold the original.\n",
    "temp_file, _ = urllib.request.urlretrieve(file_url)\n",
    "clean_file = os.path.join(data_directory, file_name)\n",
    "\n",
    "# Read the temporary file and write the cleaned up data to the permanent file.\n",
    "with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n",
    "    with tf.gfile.Open(clean_file, 'w') as eval_file:\n",
    "      for line in temp_eval_file:\n",
    "        line = line.strip()\n",
    "        line = line.replace(', ', ',')\n",
    "        if not line or ',' not in line:\n",
    "          continue\n",
    "        if line[-1] == '.':\n",
    "          line = line[:-1]\n",
    "        line += '\\n'\n",
    "        eval_file.write(line)\n",
    "tf.gfile.Remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Engineering and Exploration  \n",
    "Using a combination of pandas and matplotlib, we are able to parse our csv into a Dataframe object and visualize the data in the form of tables and plots.  \n",
    "\n",
    "First, we load the data into a Dataframe and display a sample of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>n_score</th>\n",
       "      <th>e_score</th>\n",
       "      <th>o_score</th>\n",
       "      <th>a_score</th>\n",
       "      <th>c_score</th>\n",
       "      <th>...</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legal_h</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushroom</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>0.12600</td>\n",
       "      <td>0.31287</td>\n",
       "      <td>-0.57545</td>\n",
       "      <td>-0.58331</td>\n",
       "      <td>-0.91699</td>\n",
       "      <td>-0.00665</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>1.43533</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>-0.14277</td>\n",
       "      <td>...</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL3</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL4</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>0.80523</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-1.62090</td>\n",
       "      <td>-1.01450</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.14882</td>\n",
       "      <td>-0.80615</td>\n",
       "      <td>-0.01928</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.58489</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>1.98437</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.73545</td>\n",
       "      <td>-1.63340</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.30612</td>\n",
       "      <td>...</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL1</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL2</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.59171</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-1.22751</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.67825</td>\n",
       "      <td>-0.30033</td>\n",
       "      <td>-1.55521</td>\n",
       "      <td>2.03972</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.09449</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>-0.57009</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.46725</td>\n",
       "      <td>-1.09207</td>\n",
       "      <td>-0.45174</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>0.93949</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>-1.73790</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-1.32828</td>\n",
       "      <td>1.93886</td>\n",
       "      <td>-0.84732</td>\n",
       "      <td>-0.30172</td>\n",
       "      <td>1.63088</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.48246</td>\n",
       "      <td>-0.05921</td>\n",
       "      <td>0.24923</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>0.62967</td>\n",
       "      <td>2.57309</td>\n",
       "      <td>-0.97631</td>\n",
       "      <td>0.76096</td>\n",
       "      <td>1.13407</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.82213</td>\n",
       "      <td>-0.48246</td>\n",
       "      <td>1.16365</td>\n",
       "      <td>0.96082</td>\n",
       "      <td>-0.31685</td>\n",
       "      <td>-0.24649</td>\n",
       "      <td>0.00332</td>\n",
       "      <td>-1.42424</td>\n",
       "      <td>0.59042</td>\n",
       "      <td>0.12331</td>\n",
       "      <td>...</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         age    gender education   country ethnicity   n_score   e_score  \\\n",
       "id                                                                         \n",
       "1    0.49788   0.48246  -0.05921   0.96082   0.12600   0.31287  -0.57545   \n",
       "2   -0.07854  -0.48246   1.98437   0.96082  -0.31685  -0.67825   1.93886   \n",
       "3    0.49788  -0.48246  -0.05921   0.96082  -0.31685  -0.46725   0.80523   \n",
       "4   -0.95197   0.48246   1.16365   0.96082  -0.31685  -0.14882  -0.80615   \n",
       "5    0.49788   0.48246   1.98437   0.96082  -0.31685   0.73545  -1.63340   \n",
       "6    2.59171   0.48246  -1.22751   0.24923  -0.31685  -0.67825  -0.30033   \n",
       "7    1.09449  -0.48246   1.16365  -0.57009  -0.31685  -0.46725  -1.09207   \n",
       "8    0.49788  -0.48246  -1.73790   0.96082  -0.31685  -1.32828   1.93886   \n",
       "9    0.49788   0.48246  -0.05921   0.24923  -0.31685   0.62967   2.57309   \n",
       "10   1.82213  -0.48246   1.16365   0.96082  -0.31685  -0.24649   0.00332   \n",
       "\n",
       "     o_score   a_score   c_score ...  ecstasy heroin ketamine legal_h  lsd  \\\n",
       "id                               ...                                         \n",
       "1   -0.58331  -0.91699  -0.00665 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "2    1.43533   0.76096  -0.14277 ...      CL4    CL0      CL2     CL0  CL2   \n",
       "3   -0.84732  -1.62090  -1.01450 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "4   -0.01928   0.59042   0.58489 ...      CL0    CL0      CL2     CL0  CL0   \n",
       "5   -0.45174  -0.30172   1.30612 ...      CL1    CL0      CL0     CL1  CL0   \n",
       "6   -1.55521   2.03972   1.63088 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "7   -0.45174  -0.30172   0.93949 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "8   -0.84732  -0.30172   1.63088 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "9   -0.97631   0.76096   1.13407 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "10  -1.42424   0.59042   0.12331 ...      CL0    CL0      CL0     CL0  CL0   \n",
       "\n",
       "   meth mushroom nicotine semer  vsa  \n",
       "id                                    \n",
       "1   CL0      CL0      CL2   CL0  CL0  \n",
       "2   CL3      CL0      CL4   CL0  CL0  \n",
       "3   CL0      CL1      CL0   CL0  CL0  \n",
       "4   CL0      CL0      CL2   CL0  CL0  \n",
       "5   CL0      CL2      CL2   CL0  CL0  \n",
       "6   CL0      CL0      CL6   CL0  CL0  \n",
       "7   CL0      CL0      CL6   CL0  CL0  \n",
       "8   CL0      CL0      CL0   CL0  CL0  \n",
       "9   CL0      CL0      CL6   CL0  CL0  \n",
       "10  CL0      CL0      CL6   CL0  CL0  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The column names for our csv data.\n",
    "csv_columns = [\n",
    "    'id', 'age', 'gender', 'education', 'country', 'ethnicity', 'n_score',\n",
    "    'e_score', 'o_score', 'a_score', 'c_score', 'impulsive', 'ss', 'alcohol', \n",
    "    'amphet', 'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'coke', 'crack', \n",
    "    'ecstasy', 'heroin', 'ketamine', 'legal_h', 'lsd', 'meth', 'mushroom', \n",
    "    'nicotine', 'semer', 'vsa'\n",
    "] \n",
    "\n",
    "\n",
    "# Read the entire csv (no header), apply column names to the data, and use the 'id' column as an index. \n",
    "# Force String type on all records.\n",
    "\n",
    "data = pd.read_csv(clean_file, header=None, names=csv_columns, index_col='id', dtype=str)\n",
    "\n",
    "# Display the top 10 records.\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dataframe contains 3 types of data: demographic information, psychological personality scores, and drug-use survey answers.\n",
    "\n",
    "In this case, we are interested in observing the potential abuse of heroin. We define the presence of a risk of heroin abuse as having claimed to have used heroin within the last year.\n",
    "\n",
    "To have this reflect in our Dataframe, we map each response to a 0 (representing no presence of risk of abuse) or 1 (representing presence of a risk of abuse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_map = {\n",
    "    \"CL0\" : 0, # Never Used\n",
    "    \"CL1\" : 0, # Used over a Decade Ago\n",
    "    \"CL2\" : 0, # Used in Last Decade\n",
    "    \"CL3\" : 1, # Used in Last Year\n",
    "    \"CL4\" : 1, # Used in Last Month\n",
    "    \"CL5\" : 1, # Used in Last Week\n",
    "    \"CL6\" : 1 # Used in Last Day\n",
    "}\n",
    "\n",
    "# If a given key is present in the risk_map, return its value. Otherwise, default to 0.\n",
    "def assess_risk(key):\n",
    "    if key in risk_map:\n",
    "        return risk_map[key]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Reassign the values of the 'heroin' column to their corresponding value in the risk_map.\n",
    "data['heroin'] = data['heroin'].apply(assess_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to perform any last data engineering tasks before running creating our train and test data sets. Below, we specify many lookup maps for the various categorical features. These maps are used to convert the numerical representation of the data to its contextual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that takes a lookup map and a key and returns the value of the key, or None if the key is not present.\n",
    "def lookup(lookup_map, key):\n",
    "    if key in lookup_map:\n",
    "        return lookup_map[key]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "age_map = {\n",
    "    '-0.95197':'18-24',\n",
    "    '-0.07854':'25-34',\n",
    "    '0.49788':'35-44',\n",
    "    '1.09449':'45-54',\n",
    "    '1.82213':'55-64',\n",
    "    '2.59171':'65+'\n",
    "}\n",
    "\n",
    "gender_map = {\n",
    "    '0.48246': 'f',\n",
    "    '-0.48246': 'm'\n",
    "}\n",
    "\n",
    "education_map = {\n",
    "    '-2.43591': 'Left school before 16 years',\n",
    "    '-1.73790': 'Left school at 16 years',\n",
    "    '-1.43719': 'Left school at 17 years',\n",
    "    '-1.22751': 'Left school at 18 years',\n",
    "    '-0.61113': 'Some college or university, no certificate or degree',\n",
    "    '-0.05921': 'Professional certificate/ diploma',\n",
    "    '0.45468': 'University degree',\n",
    "    '1.16365': 'Masters degree',\n",
    "    '1.98437': 'Doctorate degree'\n",
    "}\n",
    "\n",
    "country_map = {\n",
    "    '-0.09765': 'Australia',\n",
    "    '0.24923': 'Canada',\n",
    "    '-0.46841': 'New Zealand',\n",
    "    '-0.28519': 'Other',\n",
    "    '0.21128': 'Republic of Ireland',\n",
    "    '0.96082': 'UK',\n",
    "    '-0.57009': 'USA'\n",
    "}\n",
    "\n",
    "ethnicity_map = {\n",
    "    '-0.50212': 'Asian',\n",
    "    '-1.10702': 'Black',\n",
    "    '1.90725': 'Mixed-Black/Asian',\n",
    "    '0.12600': 'Mixed-White/Asian',\n",
    "    '-0.22166': 'Mixed-White/Black',\n",
    "    '0.11440': 'Other',\n",
    "    '-0.31685': 'White'\n",
    "}\n",
    "\n",
    "consumption_map = {\n",
    "    \"CL0\": \"Never Used\",\n",
    "    \"CL1\": \"Used over a Decade Ago\",\n",
    "    \"CL2\": \"Used in Last Decade\",\n",
    "    \"CL3\": \"Used in Last Year\",\n",
    "    \"CL4\": \"Used in Last Month\",\n",
    "    \"CL5\": \"Used in Last Week\",\n",
    "    \"CL6\": \"Used in Last Day\"\n",
    "}\n",
    "\n",
    "data['age'] = data['age'].apply(lambda x: lookup(age_map, x))\n",
    "data['gender'] = data['gender'].apply(lambda x: lookup(gender_map, x))\n",
    "data['education'] = data['education'].apply(lambda x: lookup(education_map, x))\n",
    "data['country'] = data['country'].apply(lambda x: lookup(country_map, x))\n",
    "data['ethnicity'] = data['ethnicity'].apply(lambda x: lookup(ethnicity_map, x))\n",
    "data['alcohol'] = data['alcohol'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['amphet'] = data['amphet'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['amyl'] = data['amyl'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['benzos'] = data['benzos'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['caff'] = data['caff'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['cannabis'] = data['cannabis'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['choc'] = data['choc'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['ketamine'] = data['ketamine'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['nicotine'] = data['nicotine'].apply(lambda x: lookup(consumption_map, x))\n",
    "data['vsa'] = data['vsa'].apply(lambda x: lookup(consumption_map, x))\n",
    "data.drop('coke', axis=1, inplace=True)\n",
    "data.drop('crack', axis=1, inplace=True)\n",
    "data.drop('ecstasy', axis=1, inplace=True)\n",
    "data.drop('legal_h', axis=1, inplace=True)\n",
    "data.drop('lsd', axis=1, inplace=True)\n",
    "data.drop('meth', axis=1, inplace=True)\n",
    "data.drop('semer', axis=1, inplace=True)\n",
    "data.drop('mushroom', axis=1, inplace=True)\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our 'heroin' column properly labeled, and our data fields converted to something a little more readable, we can now do some analysis. Below are some aggregations that allow us to see how the distribution of at-risk people across various demographic groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = data.groupby(['age', 'heroin'])['age'].count()\n",
    "gender = data.groupby(['gender', 'heroin'])['gender'].count()\n",
    "country = data.groupby(['country', 'heroin'])['country'].count()\n",
    "ethnicity = data.groupby(['ethnicity', 'heroin'])['ethnicity'].count()\n",
    "education = data.groupby(['education', 'heroin'])['education'].count()\n",
    "\n",
    "print(age, \"\\n\")\n",
    "print(gender, \"\\n\")\n",
    "print(country, \"\\n\")\n",
    "print(ethnicity, \"\\n\")\n",
    "print(education)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By making use of matplotlib, we are able to take data filters and aggregations and create visualiations to provide a better understanding of what the populations look like. In this case, we generate some bar graphs. For each graph, we declare the number of groups, perform some filtering to obtain the values of the groups we desire, assign bars to each group, and finally plot the graph out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_groups = 2\n",
    "\n",
    "# Collect Men Data\n",
    "men_at_risk = data['gender'][data['gender'] == 'm'][data['heroin'] == 1].count()\n",
    "men_no_risk = data['gender'][data['gender'] == 'm'][data['heroin'] == 0].count()\n",
    "men = (men_at_risk, men_no_risk)\n",
    "\n",
    "# Collect Women Data\n",
    "women_at_risk = data['gender'][data['gender'] == 'f'][data['heroin'] == 1].count()\n",
    "women_no_risk = data['gender'][data['gender'] == 'f'][data['heroin'] == 0].count()\n",
    "women = (women_at_risk, women_no_risk)\n",
    "\n",
    "fig, gender_plot = plt.subplots()\n",
    "\n",
    "# Assign Bars\n",
    "gender_ind = np.arange(gender_groups)\n",
    "width = 0.35\n",
    "\n",
    "men_bar = gender_plot.bar(gender_ind, men, width, color='b')\n",
    "women_bar = gender_plot.bar(gender_ind + width, women, width, color='r')\n",
    "\n",
    "# Plot Configuration\n",
    "gender_plot.set_ylabel('Population')\n",
    "gender_plot.set_title('Population Count By Risk and Gender')\n",
    "gender_plot.set_xticks(gender_ind + width / 2)\n",
    "gender_plot.set_xticklabels(('At Risk', 'No Risk'))\n",
    "gender_plot.legend((men_bar[0], women_bar[0]), ('Men', 'Women'))\n",
    "\n",
    "# Show Plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we split our data up into train and test data. This split data will be used to train and validate our Tensorflow model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = 'drug.data'\n",
    "train_file = \"%s/%s\" % (data_directory, train_file_name)\n",
    "\n",
    "test_file_name = 'drug.test'\n",
    "test_file = \"%s/%s\" % (data_directory, test_file_name)\n",
    "\n",
    "# Randomly Sample 70% of the Data to serve as Training Data.\n",
    "train = data.sample(n = int(len(data.index) * .7))\n",
    "\n",
    "# Remove all samples selected as Training Data to get the remaining Test Data.\n",
    "test = data.drop(train.index)\n",
    "\n",
    "print(\"train_count: %s\\ntesting_count: %s\" % (len(train.index), len(test.index)))\n",
    "\n",
    "# Calculate number of Positive and Negative events for Training and Test Data.\n",
    "train_positives = len(train[train['heroin'].isin([1])].index)\n",
    "train_negatives = len(train.index) - train_positives\n",
    "test_positives = len(test[test['heroin'].isin([1])].index)\n",
    "test_negatives = len(test.index) - test_positives\n",
    "\n",
    "print(\"train_positives: %s\\ntrain_negatives: %s\" % (train_positives, train_negatives))\n",
    "print(\"test_positives: %s\\ntest_negatives: %s\" % (test_positives, test_negatives))\n",
    "\n",
    "train.to_csv(train_file, header=False)\n",
    "test.to_csv(test_file, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Tensorflow Model  \n",
    "With all of our data engineering out of the way, we are ready to set up our tensors and train a Tensorflow model to perform predictions.\n",
    "\n",
    "Similar to when we first downloaded and engineered our data, we specify some values that will be used to manage our model, including its location and some configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable GPU usage as we do not need it for our model.\n",
    "run_config = tf.estimator.RunConfig().replace(\n",
    "  session_config=tf.ConfigProto(device_count={'GPU': 0}))\n",
    "\n",
    "# Directory where the model will be stored.\n",
    "model_directory = '/tmp/drug_model'\n",
    "\n",
    "# Specify number of forward and back passes over the whole training set.\n",
    "train_epochs = 40\n",
    "\n",
    "# Specify number of passes prior to each evaluation.\n",
    "epochs_per_eval = 2\n",
    "\n",
    "# Specify number of training instances to pass over at a time.\n",
    "batch_size = 20\n",
    "\n",
    "# Specify number of hidden layers and nodes in the Deep Neural Network Model.\n",
    "hidden_units = [100, 75, 50, 25]\n",
    "\n",
    "# Specify number of examples for each set of data.\n",
    "num_examples = {\n",
    "    'train': 1319,\n",
    "    'validation': 566,\n",
    "}\n",
    "\n",
    "# Specify remaining columns after our data clean up process.\n",
    "train_columns = [\n",
    "    'id', 'age', 'gender', 'education', 'country', 'ethnicity', 'n_score', 'e_score',\n",
    "    'o_score', 'a_score', 'c_score', 'impulsive', 'ss', 'alcohol', 'amphet',\n",
    "    'amyl', 'benzos', 'caff', 'cannabis', 'choc', 'heroin', 'ketamine', 'nicotine', 'vsa']\n",
    "\n",
    "# Specify default vlaues for missing values for each column.\n",
    "train_column_defaults = [\n",
    "    [''], [''], [''], [''], [''], [''], [0.0], [0.0], \n",
    "    [0.0], [0.0], [0.0], [0.0], [0.0], [''], [''], \n",
    "    [''], [''], [''], [''], [''], [''], [''], [''], ['']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some of the model configuration out of the way, we can begin setting up our Tensors. For a long time, I couldn't wrap my head around what the word \"Tensor\" really meant, but now I have realized it's just a really fancy word for a Feature that has been specified to pass through a Tensorflow model. A little unnecessary, but hey, I didn't write the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features to pass into the Linear Model.\n",
    "age = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'age', [\n",
    "      '18-24','25-34','35-44','45-54','55-64','65+'])\n",
    "\n",
    "gender = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'gender', [\n",
    "      'f', 'm'])\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'education', [\n",
    "      'Left school before 16 years', 'Left school at 16 years',\n",
    "      'Left school at 17 years', 'Left school at 18 years',\n",
    "      'Some college or university, no certificate or degree',\n",
    "      'Professional certificate/ diploma', 'University degree',\n",
    "      'Masters degree', 'Doctorate degree'])\n",
    "\n",
    "country = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'country', [\n",
    "      'Australia', 'Canada', 'New Zealand', 'Other', \n",
    "      'Republic of Ireland', 'UK', 'USA'])\n",
    "\n",
    "ethnicity = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'ethnicity', [\n",
    "      'Asian', 'Black', 'Mixed-Black/Asian', 'Mixed-White/Asian', \n",
    "      'Mixed-White/Black', 'Other', 'White'])\n",
    "\n",
    "alcohol = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'alcohol', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "amphet = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'amphet', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "amyl = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'amyl', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "benzos = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'benzos', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "caff = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'caff', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "cannabis = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'cannabis', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "choc = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'choc', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "ketamine = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'ketamine', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "vsa = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "  'vsa', [\n",
    "      'Never Used', 'Used over a Decade Ago', 'Used in Last Decade',\n",
    "      'Used in Last Year', 'Used in Last Month', 'Used in Last Week',\n",
    "      'Used in Last Day'])\n",
    "\n",
    "base_columns = [\n",
    "  age, gender, education, country, ethnicity, alcohol, amphet, \n",
    "  amyl, benzos, caff, cannabis, choc, ketamine]\n",
    "\n",
    "# Crossed Columns allow us to create cross products of Categorical Feature Columns.\n",
    "crossed_columns = [\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['age', 'education'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['age', 'education', 'ethnicity'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['education', 'country'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'education'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'country'], hash_bucket_size=1000),\n",
    "  tf.feature_column.crossed_column(\n",
    "      ['ethnicity', 'country', 'education'], hash_bucket_size=1000)]\n",
    "\n",
    "# Numerical Features to be passed into the Deep Neural Network Model.\n",
    "n_score = tf.feature_column.numeric_column('n_score')\n",
    "\n",
    "e_score = tf.feature_column.numeric_column('e_score')\n",
    "\n",
    "o_score = tf.feature_column.numeric_column('o_score')\n",
    "\n",
    "a_score = tf.feature_column.numeric_column('a_score')\n",
    "\n",
    "c_score = tf.feature_column.numeric_column('c_score')\n",
    "\n",
    "impulsive = tf.feature_column.numeric_column('impulsive')\n",
    "\n",
    "ss = tf.feature_column.numeric_column('ss')\n",
    "\n",
    "# The final input for our Linear Model.\n",
    "wide_columns = base_columns + crossed_columns\n",
    "\n",
    "# The final input for our Deep Neural Network Model.\n",
    "deep_columns = [\n",
    "  n_score, e_score, o_score, a_score, c_score, impulsive, ss]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our configuration set and our columns specified, we are ready to create our TensorFlow model object. We use the [`DNNLinearCombinedClassifier`](https://www.tensorflow.org/api_docs/python/tf/estimator/DNNLinearCombinedClassifier) in order to perform the combination of Wide and Deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.estimator.DNNLinearCombinedClassifier(\n",
    "        model_dir=model_directory,\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=hidden_units,\n",
    "        config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step prior to running our TensorFlow Learning Algorithm is to create an input function. This input function will handle everything from loading the data to be used, shuffling it after every evaluation, and extracting the features and labels from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input function for training and evaluating.\n",
    "def input_fn(run_type, data_file, shuffle, num_epochs, batch_size):\n",
    "  \n",
    "  # Check to see if the file specified exists.\n",
    "  assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found.' % data_file)\n",
    "\n",
    "  # Extract Text Data\n",
    "  dataset = tf.contrib.data.TextLineDataset(data_file)\n",
    "\n",
    "  if run_type == shuffle:\n",
    "    dataset = dataset.shuffle(buffer_size=num_examples[run_type])\n",
    "\n",
    "  # Convert our text data into features and labels.\n",
    "  def parse_csv(value):\n",
    "    print('Parsing', data_file)\n",
    "    columns = tf.decode_csv(value, record_defaults=train_column_defaults)\n",
    "    features = dict(zip(train_columns, columns))\n",
    "    labels = features.pop('heroin')\n",
    "    return features, tf.equal(labels, '1')\n",
    "    \n",
    "  dataset = dataset.map(parse_csv)\n",
    "\n",
    "  # Repeat the dataset for the number of epochs we are running.\n",
    "  dataset = dataset.repeat(num_epochs)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "\n",
    "  # Create a one-use iterator to enable TensorFlow to pass through the data.\n",
    "  iterator = dataset.make_one_shot_iterator()\n",
    "  features, labels = iterator.get_next()\n",
    "  return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, but certainly not least, we wipe out our model directory and start training a brand new model using our input functions and our configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean up the model directory if present\n",
    "shutil.rmtree(model_directory, ignore_errors=True)\n",
    "\n",
    "# Train and evaluate the model every `epochs_per_eval` epochs.\n",
    "for n in range(train_epochs // epochs_per_eval):\n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        run_type = 'train',\n",
    "        data_file = train_file, \n",
    "        shuffle = True,\n",
    "        num_epochs = epochs_per_eval, \n",
    "        batch_size = batch_size))\n",
    "\n",
    "    results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        run_type = 'test',\n",
    "        data_file = test_file, \n",
    "        shuffle = False,\n",
    "        num_epochs = 1,\n",
    "        batch_size = batch_size))\n",
    "\n",
    "    # Display evaluation metrics.\n",
    "    print('Results at epoch', (n + 1) * epochs_per_eval)\n",
    "    print('-' * 60)\n",
    "\n",
    "    for key in sorted(results):\n",
    "      print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We're done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
